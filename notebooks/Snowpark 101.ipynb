{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11daed7e",
   "metadata": {},
   "source": [
    "# Introduction to Snowpark for Pythom\n",
    "\n",
    "A simple demo of how Snowpark for Python can be used.\n",
    "\n",
    "You need to load the Campaign spend data, see 00_load_demo_data notebook, and have creds.json file with your credetials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d500ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark\n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00181444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8453c",
   "metadata": {},
   "source": [
    "Create a connection to Snowflake, Snowpark supports the following authentification methods:\n",
    "* Username and password\n",
    "* externalbrowser (Okta, ADFS, or any other SAML 2.0-compliant identity provider (IdP))\n",
    "* oauth\n",
    "* Key pair\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba59d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ff32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf6e9e-d844-4e0f-a009-6dad1c91f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_schema = session.get_fully_qualified_current_schema()\n",
    "connect_role = session.get_current_role()\n",
    "connect_wh = session.get_current_warehouse()\n",
    "\n",
    "print(f\"Current schema: {connect_schema}\")\n",
    "print(f\"Current role: {connect_role}\")\n",
    "print(f\"Current warehouse: {connect_wh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bc788",
   "metadata": {},
   "source": [
    "The parameters provided for creating a session sets the context ie database, schema, viritual warehouse and role. This can be changed using the **use_** function on the session object\n",
    "\n",
    "Chaning the active schema to INFORMATION_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e43cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_schema(\"INFORMATION_SCHEMA\")\n",
    "session.get_fully_qualified_current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fdf5e",
   "metadata": {},
   "source": [
    "We can write SQL using **sql** function on the session object, if we want the SQL to execute on Sowflake we need to use a action method like **show()** or **collect()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"SHOW WAREHOUSES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d02980",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(\"< A NAME FROM THE OUTPUT OF ABOVE >\")\n",
    "session.get_current_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202108a7-7a90-4a35-b3f4-af770ee3c64b",
   "metadata": {},
   "source": [
    "Set schema and WH to the ones we defined in our connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(f\"{connect_wh}\")\n",
    "session.use_schema(f\"{connect_schema}\")\n",
    "print(f\"Current schema: {session.get_fully_qualified_current_schema()}, current warehouse:  {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ec609",
   "metadata": {},
   "source": [
    "Define a Snowpark Dataframe based on a exsiting table/view in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18812838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend = session.table(\"CAMPAIGN_SPEND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16416410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c310b7c",
   "metadata": {},
   "source": [
    "Aggregating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e623e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spend_yearly = df_campaign_spend.group_by(F.year(\"DATE\"), \"CHANNEL\").sum(\"TOTAL_COST\").sort(\"YEAR(DATE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02575f81",
   "metadata": {},
   "source": [
    "Using the **sqlparse** libary to generate nicer print of the dataframe sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqlparse.format(df_spend_yearly.queries['queries'][0], reindent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ce635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spend_yearly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145aeec",
   "metadata": {},
   "source": [
    "If we would like to plot the data we need to use a third-party library like seaborn that supports Pandas Dataframe as input.\n",
    "A Pandas dataframe can be created with SNowpark using the **to_pandas** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = df_spend_yearly.to_pandas()\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "g = sns.barplot(\n",
    "    data=pd_data,\n",
    "    x=\"YEAR(DATE)\", y=\"SUM(TOTAL_COST)\", hue=\"CHANNEL\", ax=ax\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d701954",
   "metadata": {},
   "source": [
    "Of course can multiple agregation be done at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.group_by(F.year(\"DATE\"), \"CHANNEL\").agg([F.sum(\"TOTAL_COST\").as_(\"TOTAL_COST\"),\n",
    "                                                           F.avg(\"TOTAL_COST\").as_(\"AVG_COST\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf55658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.group_by(F.year(\"DATE\"), \"CHANNEL\").agg([F.sum(\"TOTAL_COST\").as_(\"TOTAL_COST\"), \n",
    "                                                           F.avg(\"TOTAL_COST\").as_(\"AVG_COST\")])\\\n",
    "                    .filter(F.col(\"AVG_COST\") > 840.7).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567cb7b",
   "metadata": {},
   "source": [
    "To summarise a column for the whole table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.select(F.sum(\"TOTAL_COST\").as_(\"TOTAL_COST\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fe7a9",
   "metadata": {},
   "source": [
    "We can by using Window functions calculate things like running sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_window = Window.orderBy(F.col(\"DATE\")).rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW)\n",
    "\n",
    "df_cs_running = df_campaign_spend.with_column(\"RUNNING_SUM\", F.sum(\"TOTAL_COST\").over(running_window))\n",
    "df_cs_running.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs_running.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfb9a7",
   "metadata": {},
   "source": [
    "Or using lag to get values from rows before the current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_spend.with_column(\"PREVIOUS_MONTH_TOTAL_COST\", F.lag(F.col(\"TOTAL_COST\"), 1).over(Window.orderBy(F.col(\"DATE\"))))\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d6ef1",
   "metadata": {},
   "source": [
    "Cross database joins works the same as with SQL in Snowflake, even with data from the Snowflake Marketplace!\n",
    "\n",
    "Before running below you need to go to Snowsight->Marketplace\n",
    "* Search for \"Global Weather & Climate Data for BI\"\n",
    "* Click on get\n",
    "* Click options\n",
    "* Set Database name: WEATHER and an at least the PUBLIC role \n",
    "* Click GET and Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = session.table(\"WEATHER.STANDARD_TILE.HISTORY_DAY\").filter(F.col(\"POSTAL_CODE\") == \"30170\")\n",
    "df_weather.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47415a17-9997-4489-9c4a-ea4ec73c6622",
   "metadata": {},
   "source": [
    "Add a new column that converts AVG_TEMPERATURE_AIR_2M_F to Celcius "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e36b17-633c-4470-9e29-14b58f45f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_new_col = df_weather.with_column(\"AVG_TEMPERATURE_AIR_2M_C\", F.round((F.col(\"AVG_TEMPERATURE_AIR_2M_F\") - F.lit(32)) *  F.lit(5)/F.lit(9), 2))\n",
    "df_weather_new_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184772c-146f-4cf0-9d36-fa96c8dc4c1e",
   "metadata": {},
   "source": [
    "Aggregate by day, in some cases there might be multiple reads per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_day = df_weather_new_col.group_by(F.col(\"DATE_VALID_STD\")).agg(F.avg(F.col(\"AVG_TEMPERATURE_AIR_2M_C\")).as_(\"AVG_TEMPERATURE\"))\n",
    "df_weather_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40c08a-e270-49c7-9f55-d5e71d455ff6",
   "metadata": {},
   "source": [
    "Join aggregated weather data with campaign spend data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_campaign_weather =  df_campaign_spend.join(df_weather_day, F.col(\"DATE\") == F.col(\"DATE_VALID_STD\"))\n",
    "df_campaign_weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqlparse.format(df_campaign_weather.queries['queries'][0], reindent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c406a",
   "metadata": {},
   "source": [
    "We can add new columns to a dataframe that returns diffrent values based on conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_col = df_campaign_weather.with_column(\"IS_IT_COLD\", F.iff(F.col(\"AVG_TEMPERATURE\") < 0, \"Brrrr!\", \"Not really\"))\n",
    "df_new_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386b29c",
   "metadata": {},
   "source": [
    "Every thing we have done is only logical changes and are represented by a SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c891741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqlparse.format(df_new_col.queries['queries'][0], reindent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b554c",
   "metadata": {},
   "source": [
    "We can do more complex conditions using **when**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_new_col.group_by(\"CAMPAIGN\", \"CHANNEL\").agg(F.max(\"DATE\").as_(\"LAST_DAY\"))\\\n",
    "            .with_column(\"LAST_PERIOD\", \n",
    "                         F.when(F.datediff('month', F.col(\"LAST_DAY\"), F.current_date()) <= 5, 'L5')\\\n",
    "                        .when((F.datediff('month', F.col(\"LAST_DAY\"), F.current_date()) > 5) & (F.datediff('month', F.col(\"LAST_DAY\"), F.current_date()) <= 10), 'L10')\\\n",
    "                        .otherwise(\"LB\"))\n",
    "\n",
    "df_check.filter(F.col(\"CAMPAIGN\") == F.lit('spring_break')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b06e6d",
   "metadata": {},
   "source": [
    "Pulling back the value of a specific column to do local checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_val = df_check.filter((F.col(\"CAMPAIGN\") == F.lit('spring_break')) & (F.col(\"CHANNEL\") == F.lit('social_media'))).select(F.col(\"LAST_PERIOD\")).collect()[0][0]\n",
    "if check_val == 'L5':\n",
    "    print(f'ok, we got {check_val}')\n",
    "elif check_val == 'L10':\n",
    "    print(f'Not so ok, we got {check_val}')\n",
    "elif check_val == 'LB':\n",
    "    print(f'This is bad, we got {check_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aa7dc",
   "metadata": {},
   "source": [
    "If we want to save the data we can use **save_as_table**, if we just want to apply the logic on data using SQL we can use **create_or_replace_view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_col.write.mode(\"overwrite\").save_as_table(\"my_demo_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef26869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl = session.table(\"my_demo_table\")\n",
    "df_new_tbl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.queries['queries'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323217f1",
   "metadata": {},
   "source": [
    "Tables can be updated using Snowpark, with or without a condition (condition can be based on another dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.update({\"IS_IT_COLD\": \"Too hot!\"}, df_new_tbl[\"AVG_TEMPERATURE\"] > 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.filter(df_new_tbl[\"AVG_TEMPERATURE\"] > 24.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9877280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894df42",
   "metadata": {},
   "source": [
    "Rows can also be deleted, with and without conditions (condition can be based on another dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87199bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.delete(df_new_tbl[\"DATE\"] < \"2013-12-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_tbl.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6dd244",
   "metadata": {},
   "source": [
    "Wrap some of the logic up in a Stored Procedure in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_py_pipline(snf_session: Session, input_tbl:str, output_tbl:str) -> str:\n",
    "    # Create a Snowpark DataFrame for input_tbl\n",
    "    df_input = snf_session.table(input_tbl)\n",
    "    \n",
    "    # Get the weather data\n",
    "    df_w = snf_session.table(\"WEATHER.STANDARD_TILE.HISTORY_DAY\").filter(F.col(\"POSTAL_CODE\") == \"30170\")\n",
    "    # Create the new column\n",
    "    df_w_new_col = df_w.with_column(\"AVG_TEMPERATURE_AIR_2M_C\", F.round((F.col(\"AVG_TEMPERATURE_AIR_2M_F\") - F.lit(32)) *  F.lit(5)/F.lit(9), 2))\n",
    "    # Aggregate weather data to one reading/day\n",
    "    df_w_d = df_w_new_col.group_by(F.col(\"DATE_VALID_STD\")).agg(F.avg(F.col(\"AVG_TEMPERATURE_AIR_2M_C\")).as_(\"AVG_TEMPERATURE\"))\n",
    "    # Join with input_tbl\n",
    "    df_input_weather =  df_input.join(df_w_d, F.col(\"DATE\") == F.col(\"DATE_VALID_STD\"))\n",
    "    # The new IS_COLD column including the addtional Too hot check\n",
    "    df_output = df_input_weather.with_column(\"IS_IT_COLD\", F.when(F.col(\"AVG_TEMPERATURE\") < 0, \"Brrrr!\").when(F.col(\"AVG_TEMPERATURE\") > 25, \"Too hot!\").otherwise( \"Not really\"))\n",
    "    \n",
    "    # Save it to the output_tbl, overwrite will remove existing data\n",
    "    df_output.write.mode(\"overwrite\").save_as_table(output_tbl)\n",
    "    \n",
    "    return f\"Created {output_tbl} using data from {input_tbl}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29e308",
   "metadata": {},
   "source": [
    "Since we are creating a premanent Stored Procedure from Snowpark API we need a stage for stroing the generated bytecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_name = \"ST_DEMO_101\"\n",
    "# Create a internal staging area for uploading the source file\n",
    "session.sql(f\"CREATE or replace STAGE {stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d0b79",
   "metadata": {},
   "source": [
    "Create the Stored Procedure in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "session.add_packages('snowflake-snowpark-python')\n",
    "sp_py_pipeline = F.sproc(func=my_py_pipline, name=\"sp_py_pipeline\", is_permanent = True\n",
    "                         , replace= True, stage_location = stage_name, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_py_pipeline(session, 'CAMPAIGN_SPEND', 'CAMPAIGN_SPEND_TRANSFORMED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"CAMPAIGN_SPEND_TRANSFORMED\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9463b",
   "metadata": {},
   "source": [
    "### Loading data from files into Snowflake using Snowpark\n",
    "\n",
    "Files can be loaded into tables in Snowflake using Snowpark by primary two ways\n",
    "* Load the file into a Pandas Dataframe and use **write_pandas** to load it to a table\n",
    "* Upload the file to a stage (external & internal) and load it to Snowflake\n",
    "\n",
    "Loading a file using a stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "\n",
    "# Upload the source file to the stage\n",
    "putResult = session.file.put(f\"{data_path}fraud_transactions.csv\", f\"@{stage_name}\", auto_compress=True, overwrite=True)\n",
    "\n",
    "putResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72775a24",
   "metadata": {},
   "source": [
    "In order to load a CSV file a schema needs to be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096640a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_schema is used to read from CSV files. For other files it's not needed.\n",
    "dfCustTrxFraudSchema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"TRANSACTION_ID\", T.IntegerType()),\n",
    "        T.StructField(\"TX_DATETIME\", T.TimestampType()),\n",
    "        T.StructField(\"CUSTOMER_ID\", T.IntegerType()),\n",
    "        T.StructField(\"TERMINAL_ID\", T.IntegerType()),\n",
    "        T.StructField(\"TX_AMOUNT\", T.FloatType()),\n",
    "        T.StructField(\"TX_TIME_SECONDS\", T.IntegerType()),\n",
    "        T.StructField(\"TX_TIME_DAYS\", T.IntegerType()),\n",
    "        T.StructField(\"TX_FRAUD\", T.IntegerType()),\n",
    "        T.StructField(\"TX_FRAUD_SCENARIO\", T.IntegerType())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f86ad3",
   "metadata": {},
   "source": [
    "Using the reader to create a dataframe that reads the file on stage using the above schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete a reader\n",
    "dfReader = session.read.schema(dfCustTrxFraudSchema)\n",
    "\n",
    "# Get the data into the data frame\n",
    "dfCustTrxFraudRd = dfReader.option(\"field_delimiter\", \",\").csv(f\"@{stage_name}/fraud_transactions.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004afa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCustTrxFraudRd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in dfCustTrxFraudRd.queries['queries']:\n",
    "    print(sqlparse.format(query, reindent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d05a08",
   "metadata": {},
   "source": [
    "To save the data into a table **copy_into_table** or **save_as_table** can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"DROP TABLE IF EXISTS copied_into_table\").collect()\n",
    "copied_into_result = dfCustTrxFraudRd.copy_into_table(\"copied_into_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_into_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7436104",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"copied_into_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3ceb",
   "metadata": {},
   "source": [
    "When loading JSON data we do not need a schema since we canload it as-is into a Variant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.file.put(f\"{data_path}nutrition_tweets.json\", f\"@{stage_name}\", auto_compress=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66945f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_stage = session.read.json(f\"@{stage_name}/nutrition_tweets.json\")\n",
    "df_json_stage.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b40a09",
   "metadata": {},
   "source": [
    "We need to create the table first in order to use copy_into_table (for CSV the table is created aytomatically if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0846a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"CREATE OR REPLACE TABLE json_table (RAW VARIANT)\").collect()\n",
    "df_json_stage.copy_into_table(\"json_table\",  target_columns=[\"RAW\"], \n",
    "                              format_type_options= {\"STRIP_OUTER_ARRAY\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e325cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = session.table(\"json_table\")\n",
    "df_json.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c41727-2494-4564-8e55-6a04b86d15b4",
   "metadata": {},
   "source": [
    "To select the id and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.select(F.to_varchar(df_json[\"RAW\"][\"user\"][\"id\"]).as_(\"USER_ID\"),\n",
    "               (df_json[\"RAW\"][\"entities\"][\"hashtags\"]).as_(\"HASHTAG_TEXT\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245a87c-146f-4120-b9a9-56a9b584de10",
   "metadata": {},
   "source": [
    "To return one row for each hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_json = df_json.join_table_function(\"flatten\", df_json[\"RAW\"][\"entities\"][\"hashtags\"])\\\n",
    "        .select(F.to_varchar(df_json[\"RAW\"][\"user\"][\"id\"]).as_(\"USER_ID\"),\n",
    "               F.to_varchar(F.col(\"VALUE\")[\"text\"]).as_(\"HASHTAG_TEXT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b68368-a994-45a4-8c03-3664b2125fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
